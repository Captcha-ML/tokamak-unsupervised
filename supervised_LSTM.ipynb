{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:04.836581Z",
     "start_time": "2023-12-20T21:34:02.462296Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "save_dir = ''\n",
    "file_names = glob.glob(os.path.join(save_dir, f'QCEH_data/TCV_DATAno*.parquet'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:04.839623Z",
     "start_time": "2023-12-20T21:34:04.837423Z"
    }
   },
   "id": "4d9814df35498d66"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "precomputed_dir = 'precomputed_data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:04.842141Z",
     "start_time": "2023-12-20T21:34:04.840248Z"
    }
   },
   "id": "f928c8c1ebae3e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "List of dataframes (one discharge per each element of the list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbca450725d5581"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "60"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = [pd.read_parquet(x) for x in file_names]\n",
    "len(df_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:04.989728Z",
     "start_time": "2023-12-20T21:34:04.842449Z"
    }
   },
   "id": "befb198e4560f45a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial Analysis - necessary values\n",
    "For this project we are only focusing on the machine inputs provided in the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c22d98064cda16d0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERAL_INPUTS = [\"shotnumber\", \"time\"]\n",
    "MACHINE_INPUTS = [\"isbaffled\", \"ip\",\"b0\",\"nel\",\"ptot\",\"pdiv\",\"q95\",\"betan\",\"kappa\",\"deltaavg\",\"deltaupp\",\n",
    "                  \"deltalow\",\"gapin\",\"gapout\",\"zmag\",\"rmag\",\"rmin\",\"lpar_ot\",\"zeff\"]\n",
    "LABEL = [\"LHD_label\"]\n",
    "INPUTS = GENERAL_INPUTS + MACHINE_INPUTS + LABEL\n",
    "len(INPUTS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:04.993740Z",
     "start_time": "2023-12-20T21:34:04.991364Z"
    }
   },
   "id": "3fe0fbf211d6cf9e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(571675, 19)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This normalizes the X values.\n",
    "X_train, y_train, column_names = get_training_data(include_time = False, include_shotnumber = False, shot_indices = list(range(55)))\n",
    "X_test, y_test, _ = get_training_data(include_time = False, include_shotnumber = False, shot_indices = list(range(56, 60)))\n",
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.379502Z",
     "start_time": "2023-12-20T21:34:04.994144Z"
    }
   },
   "id": "92fd99c80f09cf77"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "D = 40\n",
    "num_train = X_train.shape[0]\n",
    "mask = np.arange(num_train) % D == 0\n",
    "X_train = X_train[mask, :]\n",
    "y_train = y_train[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.387974Z",
     "start_time": "2023-12-20T21:34:05.379094Z"
    }
   },
   "id": "3b61f7ad3c8f3a63"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_tensor_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor_train = torch.tensor((y_train - 1).astype(int), dtype=torch.long)\n",
    "\n",
    "X_tensor_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_tensor_test = torch.tensor((y_test - 1).astype(int), dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.396730Z",
     "start_time": "2023-12-20T21:34:05.389292Z"
    }
   },
   "id": "630d6c5a852385b4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/vpp5gf_96v7f5w4frq2kpfn00000gn/T/ipykernel_6284/3614208288.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features_tensor = torch.tensor(X_tensor_train, dtype=torch.float32)\n",
      "/var/folders/vq/vpp5gf_96v7f5w4frq2kpfn00000gn/T/ipykernel_6284/3614208288.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(y_tensor_train, dtype=torch.long)  # Assuming classification labels are integers\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "features_tensor = torch.tensor(X_tensor_train, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(y_tensor_train, dtype=torch.long)  # Assuming classification labels are integers\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "batch_size = 32  # You can adjust this according to your dataset size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.412102Z",
     "start_time": "2023-12-20T21:34:05.397831Z"
    }
   },
   "id": "987c9f63e81cf05e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob=0.5, noise_level=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "        # Adding dropout to LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_prob if num_layers > 1 else 0.0)\n",
    "\n",
    "        # Dropout layer before the fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add Gaussian noise to input\n",
    "        if self.training and self.noise_level > 0:\n",
    "            noise = torch.randn_like(x) * self.noise_level\n",
    "            x = x + noise\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "    \n",
    "        # Forward pass through LSTM layer\n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))\n",
    "    \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.415797Z",
     "start_time": "2023-12-20T21:34:05.402073Z"
    }
   },
   "id": "e9bb9c828cc11b63"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "input_size = 19  # Number of features per time step\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "num_classes = 3  \n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:05.555968Z",
     "start_time": "2023-12-20T21:34:05.404644Z"
    }
   },
   "id": "723bc95479bbfa1c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 1.0485\n",
      "Epoch [2/100], Training Loss: 0.0243\n",
      "Epoch [3/100], Training Loss: 0.0082\n",
      "Epoch [4/100], Training Loss: 0.0037\n",
      "Epoch [5/100], Training Loss: 0.0033\n",
      "Epoch [6/100], Training Loss: 0.0020\n",
      "Epoch [7/100], Training Loss: 0.0009\n",
      "Epoch [8/100], Training Loss: 0.0034\n",
      "Epoch [9/100], Training Loss: 0.0019\n",
      "Epoch [10/100], Training Loss: 0.0008\n",
      "Epoch [11/100], Training Loss: 0.0008\n",
      "Epoch [12/100], Training Loss: 0.0004\n",
      "Epoch [13/100], Training Loss: 0.0003\n",
      "Epoch [14/100], Training Loss: 0.0009\n",
      "Epoch [15/100], Training Loss: 0.0001\n",
      "Epoch [16/100], Training Loss: 0.0002\n",
      "Epoch [17/100], Training Loss: 0.0002\n",
      "Epoch [18/100], Training Loss: 0.0009\n",
      "Epoch [19/100], Training Loss: 0.0003\n",
      "Epoch [20/100], Training Loss: 0.0005\n",
      "Epoch [21/100], Training Loss: 0.0007\n",
      "Epoch [22/100], Training Loss: 0.0002\n",
      "Epoch [23/100], Training Loss: 0.0007\n",
      "Epoch [24/100], Training Loss: 0.0004\n",
      "Epoch [25/100], Training Loss: 0.0002\n",
      "Epoch [26/100], Training Loss: 0.0001\n",
      "Epoch [27/100], Training Loss: 0.0003\n",
      "Epoch [28/100], Training Loss: 0.0011\n",
      "Epoch [29/100], Training Loss: 0.0005\n",
      "Epoch [30/100], Training Loss: 0.0001\n",
      "Epoch [31/100], Training Loss: 0.0005\n",
      "Epoch [32/100], Training Loss: 0.0002\n",
      "Epoch [33/100], Training Loss: 0.0000\n",
      "Epoch [34/100], Training Loss: 0.0002\n",
      "Epoch [35/100], Training Loss: 0.0000\n",
      "Epoch [36/100], Training Loss: 0.0001\n",
      "Epoch [37/100], Training Loss: 0.0001\n",
      "Epoch [38/100], Training Loss: 0.0001\n",
      "Epoch [39/100], Training Loss: 0.0001\n",
      "Epoch [40/100], Training Loss: 0.0001\n",
      "Epoch [41/100], Training Loss: 0.0002\n",
      "Epoch [42/100], Training Loss: 0.0004\n",
      "Epoch [43/100], Training Loss: 0.0003\n",
      "Epoch [44/100], Training Loss: 0.0002\n",
      "Epoch [45/100], Training Loss: 0.0001\n",
      "Epoch [46/100], Training Loss: 0.0003\n",
      "Epoch [47/100], Training Loss: 0.0002\n",
      "Epoch [48/100], Training Loss: 0.0002\n",
      "Epoch [49/100], Training Loss: 0.0000\n",
      "Epoch [50/100], Training Loss: 0.0002\n",
      "Epoch [51/100], Training Loss: 0.0004\n",
      "Epoch [52/100], Training Loss: 0.0002\n",
      "Epoch [53/100], Training Loss: 0.0002\n",
      "Epoch [54/100], Training Loss: 0.0000\n",
      "Epoch [55/100], Training Loss: 0.0004\n",
      "Epoch [56/100], Training Loss: 0.0000\n",
      "Epoch [57/100], Training Loss: 0.0001\n",
      "Epoch [58/100], Training Loss: 0.0002\n",
      "Epoch [59/100], Training Loss: 0.0007\n",
      "Epoch [60/100], Training Loss: 0.0003\n",
      "Epoch [61/100], Training Loss: 0.0001\n",
      "Epoch [62/100], Training Loss: 0.0001\n",
      "Epoch [63/100], Training Loss: 0.0000\n",
      "Epoch [64/100], Training Loss: 0.0002\n",
      "Epoch [65/100], Training Loss: 0.0002\n",
      "Epoch [66/100], Training Loss: 0.0001\n",
      "Epoch [67/100], Training Loss: 0.0002\n",
      "Epoch [68/100], Training Loss: 0.0001\n",
      "Epoch [69/100], Training Loss: 0.0006\n",
      "Epoch [70/100], Training Loss: 0.0002\n",
      "Epoch [71/100], Training Loss: 0.0002\n",
      "Epoch [72/100], Training Loss: 0.0001\n",
      "Epoch [73/100], Training Loss: 0.0010\n",
      "Epoch [74/100], Training Loss: 0.0001\n",
      "Epoch [75/100], Training Loss: 0.0001\n",
      "Epoch [76/100], Training Loss: 0.0002\n",
      "Epoch [77/100], Training Loss: 0.0000\n",
      "Epoch [78/100], Training Loss: 0.0003\n",
      "Epoch [79/100], Training Loss: 0.0002\n",
      "Epoch [80/100], Training Loss: 0.0004\n",
      "Epoch [81/100], Training Loss: 0.0000\n",
      "Epoch [82/100], Training Loss: 0.0001\n",
      "Epoch [83/100], Training Loss: 0.0033\n",
      "Epoch [84/100], Training Loss: 0.0002\n",
      "Epoch [85/100], Training Loss: 0.0004\n",
      "Epoch [86/100], Training Loss: 0.0001\n",
      "Epoch [87/100], Training Loss: 0.0001\n",
      "Epoch [88/100], Training Loss: 0.0001\n",
      "Epoch [89/100], Training Loss: 0.0005\n",
      "Epoch [90/100], Training Loss: 0.0000\n",
      "Epoch [91/100], Training Loss: 0.0002\n",
      "Epoch [92/100], Training Loss: 0.0001\n",
      "Epoch [93/100], Training Loss: 0.0005\n",
      "Epoch [94/100], Training Loss: 0.0001\n",
      "Epoch [95/100], Training Loss: 0.0001\n",
      "Epoch [96/100], Training Loss: 0.0001\n",
      "Epoch [97/100], Training Loss: 0.0001\n",
      "Epoch [98/100], Training Loss: 0.0001\n",
      "Epoch [99/100], Training Loss: 0.0005\n",
      "Epoch [100/100], Training Loss: 0.0002\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    show = True\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(show):\n",
    "            show=False\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "            #print(f'Epoch [{epoch + 1}/{num_epochs}], ')\n",
    "        \n",
    "\n",
    "print('Training complete!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:28.861773Z",
     "start_time": "2023-12-20T21:34:05.557467Z"
    }
   },
   "id": "c6bb71e3cbe0ce7e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "batch_size = 32  # You can adjust this according to your dataset size\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:28.864357Z",
     "start_time": "2023-12-20T21:34:28.862547Z"
    }
   },
   "id": "a92a7232169ed520"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "def multi_confusion_matrix(y_predicted, y_expected):\n",
    "    \"\"\"\n",
    "    Function constructing a multiclass confusion matrix\n",
    "    \n",
    "    :param y_predicted: pd.Series containing the predicted labels\n",
    "    :param y_expected: pd.Series containing the expected labels\n",
    "    :return: pd.Dataframe containing the multiclass confusion matrix \n",
    "    with columns as true labels and rows are predicted labels\n",
    "    \"\"\"\n",
    "    predicted_labels = y_predicted.unique()\n",
    "    expected_labels = y_expected.unique()\n",
    "    \n",
    "    mtrx = pd.DataFrame(0, columns=expected_labels, index=expected_labels)\n",
    "    \n",
    "    for expected_label in expected_labels:\n",
    "        for predicted_label in predicted_labels:\n",
    "            count = sum((y_expected == expected_label) & (y_predicted == predicted_label))\n",
    "            mtrx.at[predicted_label, expected_label] = count\n",
    "    return mtrx\n",
    "\n",
    "def diag(matrix):\n",
    "    \"\"\"\n",
    "    Function retrieving the true values of a confusion matrix\n",
    "    \n",
    "    :param matrix: confusion matrix\n",
    "    :return: list of all the diagonal values\n",
    "    \"\"\"\n",
    "    if len(matrix.index) <= len(matrix.columns):\n",
    "        zipped = zip(matrix.index, matrix.columns[:len(matrix.index)])\n",
    "    else:\n",
    "        zipped = zip(matrix.index[:len(matrix.columns)], matrix.columns)\n",
    "    \n",
    "    diag = []\n",
    "    for idx,col in zipped:\n",
    "        diag.append(matrix.at[idx,col])\n",
    "    return diag\n",
    "\n",
    "def multi_accuracy(matrix):\n",
    "    \"\"\"\n",
    "    Function computing the accuracy associated with a confusion matrix\n",
    "    :param matrix: confusion matrix\n",
    "    :return: precision score\n",
    "    \"\"\"\n",
    "    t = sum(diag(matrix))\n",
    "    all = matrix.sum().sum()\n",
    "    return t / all\n",
    "    \n",
    "def multi_precision(matrix):\n",
    "    \"\"\"\n",
    "    Function computing the precision associated with a confusion matrix\n",
    "    :param matrix: confusion matrix\n",
    "    :return: precision score\n",
    "    \"\"\"\n",
    "    tp = diag(matrix)\n",
    "    pred_p = matrix.sum(axis=1)\n",
    "    return (tp / pred_p).fillna(0)\n",
    "def multi_recall(matrix):\n",
    "    \"\"\"\n",
    "    Function computing the recall associated with a confusion matrix\n",
    "    :param matrix: confusion matrix\n",
    "    :return: recall score\n",
    "    \"\"\"\n",
    "    tp = diag(matrix)\n",
    "    acutal_p = matrix.sum(axis=0)\n",
    "    return (tp / acutal_p).fillna(0)\n",
    "\n",
    "def multi_f1_score(matrix):\n",
    "    \"\"\"\n",
    "    Function computing the F1-score associated with a confusion matrix\n",
    "    :param matrix: confusion matrix\n",
    "    :return: F1-score\n",
    "    \"\"\"\n",
    "    recall = multi_recall(matrix)\n",
    "    precision = multi_precision(matrix)\n",
    "    \n",
    "    multiplied = recall.multiply(precision)\n",
    "    summed =recall + precision\n",
    "    return (2 * multiplied / summed).fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:28.870961Z",
     "start_time": "2023-12-20T21:34:28.869729Z"
    }
   },
   "id": "8a6edcca96dd1ce5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1    0.911356\n",
      "3    0.896329\n",
      "2    0.806564\n",
      "dtype: float64\n",
      "Accuracy: 0.8822099396830493\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for shots, labels in test_loader:\n",
    "        outputs = model(shots)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predicted.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Convert to pandas Series for compatibility with your functions\n",
    "y_predicted = pd.Series(all_predicted)\n",
    "y_true = pd.Series(all_labels)\n",
    "\n",
    "# Assuming your multi_confusion_matrix, multi_f1_score, and multi_accuracy functions accept pandas Series\n",
    "mtrx_test = multi_confusion_matrix(y_predicted + 1, y_true + 1)  # Adjust indices if necessary\n",
    "f1_score = multi_f1_score(mtrx_test)\n",
    "accuracy = multi_accuracy(mtrx_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'F1 Score: {f1_score}')\n",
    "print(f'Accuracy: {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:29.205877Z",
     "start_time": "2023-12-20T21:34:28.872927Z"
    }
   },
   "id": "fa9f1e5af0aa44ee"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "LSTMClassifier(\n  (lstm): LSTM(19, 64, batch_first=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=64, out_features=3, bias=True)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:29.208391Z",
     "start_time": "2023-12-20T21:34:29.206247Z"
    }
   },
   "id": "863a575811511c5c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "with open(f\"{precomputed_dir}model-19-64-shuffle-falseDropout4-Noise.pkl\", 'wb') as f:\n",
    "    pkl.dump(model, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:29.212559Z",
     "start_time": "2023-12-20T21:34:29.209024Z"
    }
   },
   "id": "5071a2b027f14a54"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:34:29.212624Z",
     "start_time": "2023-12-20T21:34:29.211392Z"
    }
   },
   "id": "fccc98fbe5c3c35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
